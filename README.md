# Graduation_Project
Text to Art Generation

# Introduction
The AI art apps generate images by simply typing a brief description of what we want to see, no matter what we type, the app will generate something visually compelling that matches our prompt in often surprisingly opposite ways. This sort of AI-generated artwork is becoming higher quality and more accessible. Past examples of these sorts of text-to-image models have included research-orientated programs like DALL-E and VQGAN+CLIP, as well as more specialized commercial projects like Art breeder (which is particularly good at creating portraits of fictional beings and people). Generally, programs like these are trained on vision datasets â€” huge libraries of images that are tagged based on objects and scenery.

# Scope
We focus on model architecture and learning paradigms to learn more about the strengths and weaknesses of generative models applied in text-to-image. We also try to organize these works to show development over the years. Also, we mention top-powered tools and applications that are used at present. Finally, we emphasize existing and future challenges. 

# objectives
Images are more attractive compared to text. Visual aids can deliver information more directly. Visual content grabs the attention and keeps people engaged. Key activities such as presentation, learning, and all involve visual Communication to some degree. If designed well, it offers numerous benefits.

# Generative Models
